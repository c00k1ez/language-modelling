
general:
    seed: 42
    checkpoint_path: ./bpe_lm_checkpoints/

trainer_params:
    max_epochs: 30
    val_check_interval: 0.25
    default_save_path: ./logs/
    gpus: 1


dataloaders:
    train_batch_size: 30
    test_batch_size: 20
    pad_len: 150
    tokenizer_type: bpe
    model_file: ./data/bpe_15000_vocab.model
    dropout_prob: 0.1

model:
    vocab_size: 15000
    embeddig_dim: 256 
    hidden_size: 256

optimizer:
    learning_rate: 0.001
